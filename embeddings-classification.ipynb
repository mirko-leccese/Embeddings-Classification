{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a23061-62b7-4da0-a64d-3b484098bf96",
   "metadata": {},
   "source": [
    "# Performing Classification Tasks with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba3b10b-ab71-46ea-95c8-6906bc0c55ae",
   "metadata": {},
   "source": [
    "In this notebook, we present a coincise and accessible introduction to **NLP embeddings** and delve into one of their potential applications in AI applications, namely *classification task*. As an illustrative example, we examine the case of classifying films genres based on plot descriptions, utilizing the *Hydra-Movie-Scrape* dataset sourced from [DataWorld](https://data.world/iliketurtles/movie-dataset): This notebook is structured as follows:\n",
    "- Introduction: in this section, we provide a conceptual understanding of embeddings and highlight some their most common applications;\n",
    "- Data Preparation: in this section, we conduct a brief exploration of the source dataset and undertake necessary steps to prepare the data for subsequent analysis;\n",
    "- Class Embeddings: in this section, we show how we can easily generate text embeddings using the `embeddings.create` endpoint of the OpenAI API, applying it to the available classes pertinent to our classification problem;\n",
    "- Classying Movie Genres: finally, we present a complete pipeline for the classification of movie genres using embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21f39d-f81b-4691-8d94-e5d3afb54677",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224b7f3-f84d-412e-92c7-cbd30b381e1c",
   "metadata": {},
   "source": [
    "Roughly speaking, *embeddings* are a numerical representation of words, sentence or entire pieces of text, in terms of *vectors* in high-dimensional vector spaces. Before the advent of modern algorithms in deep learning, there were already exist several algorithms to convert words into numbers. The most common is probably the so-called *Count Vectorizer* method, which works as follow. \n",
    "\n",
    "Let suppose to have a vocabulary $V$ of $N$ words, each one identified by an index $i$ and that the word \"Hello\" is at position $i=100$. Then, a represention of the word \"Hello\" can be defined as:\n",
    "$$ w^i_k = \\delta_{ik}$$\n",
    "where $\\delta_{ik}$ is the Delta Kronecker, i.e. $\\delta_{ik} = 1$ if $k=1$, otherwise 0.Despite its simplicity, this method has several drawbacks:\n",
    "- when we have a large vocabulary, vector representations of words are *sparse* in nature, leading to computational inefficiency in most applications:\n",
    "- vectorized words are all orthogonal to each others, therefore this method lacks of a *semantic* understanding of the language since similar words will still have a vanishing distance (we will return on the distance, hence *metric* definition in embedding spaces later)\n",
    "\n",
    "These limitations were addressed **Word2Vec** algorithm, which is one of the pioneering techniques for learning word embeddings. The fundamental concept behind Word2Vec, and its derived methods, is to replace the discrete and sparse word vector represention with a *dense and continuous* representation. Consequently, the representation becomes *distributed*, meaning that the word is spread across all the dimensions. Moreover, Word2Vec leverages the concept of the so-called **Distributional Semantics**, which involves understanding a word's meaning through its contextual associations, that is its *context*.\n",
    "A comprehensive description of the Word2Vec algorithm is beyond the scope of this notebook. However, it is worth briefly highlighting how it operates, particularly the Skip-Gram implementation.\n",
    "\n",
    "We consider all words $\\vec{w}$ in our vocabulary $V$ and first initiliaze vector components to real random numbers. Let then consider a piece of text, where each word occupy a position defined by the index $p$. For each position, we define the *center word* as the word at position $p$ (i.e. $\\vec{w}^p$), and the *context words* as words within the window $[p-m, p+m]$ where $m$ defines the window extent in terms of words number. The goal is to maximize the probability of the context words given the center word, in other words the probability of our model predicting the context words given the center word. This probability can be defined through the following *likelihood function*:\n",
    "$$ L(\\theta) = \\prod_p \\prod_{-m \\leq j \\leq m, j\\neq 0} P(\\vec{w}_{p+j} | \\vec{w}_{p}; \\theta)$$\n",
    "\n",
    "The first product runs over the context window, the second runs over all available positions within the text. The above function can be written in a simpler form for maximization by taking the negative $\\log$:\n",
    "$$ J(\\theta) = - \\frac{1}{n_p} \\log L(\\theta) = - \\frac{1}{n_p} \\sum_p \\sum_{-m \\leq j \\leq m, j\\neq 0}P(\\vec{w}_{p+j} | \\vec{w}_{p}; \\theta)$$\n",
    "where $n_p$ is the number of positions within the text. There is just one parameter $\\theta$ in our model equation that has to be determined in the training phase. Such parameter arises from the explicit representation of $P(w_{p+j} | w_{p})$ conditional probabilities. Indeed, these can be modeled as follows. Suppose that each word can be represented by two vectors, that we call $\\vec{w}_c$ and $\\vec{w_t}$, where the former is used when the word is a context word, while the latter when the word is a center word ($t$ stands for \"target\"). Then, the probability of observing the context word given the target word is the following softmax function:\n",
    "$$ P(\\vec{w}_c | \\vec{w}_t) = \\frac{\\exp(\\vec{w}_c^T \\vec{w}_t)}{\\sum_{p \\in V} \\vec{w}_{p, c}^T \\vec{w}_t}$$\n",
    "Basically, the numerator captures the distance between the context nd target words, i.e. their similarity. The denominator is the sum of the dot product between the target word and all words in the vocubolary and acts as a normalization constant so that probabilities all add up to 1.\n",
    "Therefore, $\\theta$ will be a vector containing all pair of vectors $\\vec{w}_c$ and $\\vec{w}_{t}$ for each word in the vocabulary. If the vocabulary size is $n_p$ and each vector lies in a $d$-dimensional vector space (the embedding space), then $\\theta \\in \\mathcal{R}^{2dn_p}$. Its component can be then learned applied a optimization algorithm such as *gradient descent*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d789d8-f326-42d6-9be1-06709f385a03",
   "metadata": {},
   "source": [
    "### Metrics in the embedding space: the cosine distance\n",
    "\n",
    "Now, let's consider that we've generated embeddings for two words using an available embedding generation algorithm. As embeddings encode word semantics, we can compute the distance between two embeddings to reflect the similarity between the original words, by introducing a metric in the embedding space. Once more, a thorough analysis of potential metrics is beyond our current scope (extensive literature exists on metric spaces!). Here, we introduce the simplest form of distance, which is suitable for most AI applications: the **cosine distance**. The cosine distance between two vectors $\\vec{u}, \\vec{v}$ is defined as:\n",
    "$$D(\\vec{u}, \\vec{v})  = 1 - S(\\vec{u}, \\vec{v})$$\n",
    "where $S(\\vec{u}, \\vec{v})$ is the *cosine similarity* i.e.:\n",
    "$$S(\\vec{u}, \\vec{v}) = \\cos(\\alpha) = \\frac{\\vec{u}^T \\vec{v}}{||\\vec{u}|| \\ || \\vec{v}||}$$\n",
    "that is the cosine of the angle between the two vectors. Since $\\cos(\\alpha) \\in [-1,1]$, the cosine distances ranges between 0 and 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b74b64-b99f-48f5-9103-398732e9fed7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f8ff; padding: 10px; border-radius: 5px; border: 1px solid #87CEEB;\">\n",
    "    <strong>Math Curiosity!:</strong> Strictly speaking, the cosine distance is NOT a metric. Indeed, the cosine distance does not satisfy the so-called Scharwz inequality, i.e. $D(\\vec{x}, \\vec{z}) <= D(\\vec{x}, \\vec{y}) + D(\\vec{x}, \\vec{z})$. Furthermore, the cosine similarity $S(\\cdot)$ itself is not a metric over $R^n$ since it is not defined whether one of the input is 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052fd5cc-7620-445a-83f8-38dcfcce1bdc",
   "metadata": {},
   "source": [
    "In the following, we will use the `distance` method of the `scipy`package to compute cosine distance between word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b6732-8ca7-43f3-8bb1-feaf195122d7",
   "metadata": {},
   "source": [
    "### Embeddings in AI Application\n",
    "\n",
    "Embeddings and their relative distances can be used in a variety of AI Applications but the most common are the followings:\n",
    "- **Semantic Search**: the user input a query which is transformed into an embedding. Such embedding is matched with available embeddings (e.g. stored in a **vector database**) and the most similar are retrieved. The foundation of the technique lies again in the *distributional hypothesis*, namely that semantically similar texts are embedded more closely in the vector space;\n",
    "- **Recommendation Engines**: a recommendation system can return the embeddings that most closely match embeddings representing user's interests. Suppose for example that we have access to the user history about his reading in an article magazine. We can average the embeddings relative to read documents to produce a \"mean embedding\" representing user's interest and match with all document embeddings to find the most similar, i.e. recommendable articles;\n",
    "- **Classification Tasks**: again, we embed the text we want to classify and match with embeddings representing the available class descriptions. This is at times called **zero-shot classification**, since, unlike traditional supervise learning algorithms for classification such as *logistic regression* or *neural networks*, we do not use already labeled data. We just exploit the semantical similarity between data and class.\n",
    "\n",
    "Here, we specifically focus on the third application and show how we can build a simple \"classifier\" based on embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6440bf-d12e-4610-b8fe-a6d9a13c9ec1",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's now proceed with coding to demonstrate how the process of generating and utilizing embeddings for classification is straightforward, thanks to the OpenAI API. First and foremost, we need to load the movie CSV file into a `pandas` dataframe and preprocess the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5534901b-7288-44b4-84e6-e765c051b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# import openai, scipy, scikit learn and other useful packages\n",
    "from openai import OpenAI\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea95526d-248c-496e-a035-be8bc44db6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dataset\n",
    "df_movie = pd.read_csv(\"Hydra-Movie-Scrape.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f0a4372-6d15-4163-aa2a-34d90750e2b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Short Summary</th>\n",
       "      <th>Genres</th>\n",
       "      <th>IMDB ID</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>YouTube Trailer</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Movie Poster</th>\n",
       "      <th>Director</th>\n",
       "      <th>Writers</th>\n",
       "      <th>Cast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patton Oswalt: Annihilation</td>\n",
       "      <td>2017</td>\n",
       "      <td>Patton Oswald, despite a personal tragedy, pro...</td>\n",
       "      <td>Patton Oswalt, despite a personal tragedy, pro...</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>tt7026230</td>\n",
       "      <td>66</td>\n",
       "      <td>4hZi5QaMBFc</td>\n",
       "      <td>7.4</td>\n",
       "      <td>https://hydramovies.com/wp-content/uploads/201...</td>\n",
       "      <td>Bobcat Goldthwait</td>\n",
       "      <td>Patton Oswalt</td>\n",
       "      <td>Patton Oswalt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New York Doll</td>\n",
       "      <td>2005</td>\n",
       "      <td>A recovering alcoholic and recently converted ...</td>\n",
       "      <td>A recovering alcoholic and recently converted ...</td>\n",
       "      <td>Documentary|Music</td>\n",
       "      <td>tt0436629</td>\n",
       "      <td>75</td>\n",
       "      <td>jwD04NsnLLg</td>\n",
       "      <td>7.9</td>\n",
       "      <td>https://hydramovies.com/wp-content/uploads/201...</td>\n",
       "      <td>Greg Whiteley</td>\n",
       "      <td>Arthur Kane</td>\n",
       "      <td>Sylvain Sylvain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title  Year  \\\n",
       "0  Patton Oswalt: Annihilation  2017   \n",
       "1                New York Doll  2005   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Patton Oswald, despite a personal tragedy, pro...   \n",
       "1  A recovering alcoholic and recently converted ...   \n",
       "\n",
       "                                       Short Summary             Genres  \\\n",
       "0  Patton Oswalt, despite a personal tragedy, pro...      Uncategorized   \n",
       "1  A recovering alcoholic and recently converted ...  Documentary|Music   \n",
       "\n",
       "     IMDB ID  Runtime YouTube Trailer  Rating  \\\n",
       "0  tt7026230       66     4hZi5QaMBFc     7.4   \n",
       "1  tt0436629       75     jwD04NsnLLg     7.9   \n",
       "\n",
       "                                        Movie Poster           Director  \\\n",
       "0  https://hydramovies.com/wp-content/uploads/201...  Bobcat Goldthwait   \n",
       "1  https://hydramovies.com/wp-content/uploads/201...      Greg Whiteley   \n",
       "\n",
       "         Writers             Cast  \n",
       "0  Patton Oswalt    Patton Oswalt  \n",
       "1    Arthur Kane  Sylvain Sylvain  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161a489-b332-4888-a652-8229d8bd2122",
   "metadata": {},
   "source": [
    "In this tutorial, we will focus mainly on three columns: **Title**, **Summary** and **Genres**. The first contains the title of the movie, the second a summary of the plot, the third the *genres*, i.e. the label we want to predict.\n",
    "\n",
    "We can see that a movie can have multiple genres, separated by a \"|\", as displayed by the movie at index 0, *New York Doll*. For simplicity, we split the Genre string based on \"|\" and build a column with a list of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a618d016-e586-4687-b959-d25e24679e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_genre(split_str: str, sep: str) -> List[str]:\n",
    "    '''\n",
    "    Function accepting a string and a separator and return the splitted string as a list\n",
    "    :param split_str: string to split\n",
    "    :param sep: separator in the string\n",
    "    :returns: the splitted string as a list\n",
    "    '''\n",
    "    return split_str.split(sep)\n",
    "\n",
    "# Apply the function to the 'Genre' column\n",
    "df_movie[\"Genres\"] = df_movie[\"Genres\"].apply(lambda x: split_genre(x, \"|\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "311419ec-249e-4db0-890f-9c66d4ddb32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       [Uncategorized]\n",
       "1                                  [Documentary, Music]\n",
       "2       [Adventure, Animation, Comedy, Family, Fantasy]\n",
       "3          [Animation, Comedy, Family, Fantasy, Horror]\n",
       "4                                               [Drama]\n",
       "                             ...                       \n",
       "3935                                 [Action, Thriller]\n",
       "3936                            [Horror, Thriller, War]\n",
       "3937                            [Comedy, Drama, Family]\n",
       "3938                                           [Comedy]\n",
       "3939                         [Action, Sci-Fi, Thriller]\n",
       "Name: Genres, Length: 3940, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct genres available in the dataset\n",
    "df_movie[\"Genres\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d098b49-0bcd-4959-a127-1762e468bc0e",
   "metadata": {},
   "source": [
    "Let identifiy how many distinc genres we have in our dataset. These will become the potential class of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d58f924f-eb9c-45f2-aba1-0f4d5b089bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FInd distinct genres\n",
    "distinct_genres = df_movie.explode(\"Genres\")[\"Genres\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c390b8f7-4edd-4a6a-b2d2-95715b6b5527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Uncategorized', 'Documentary', 'Music', 'Adventure', 'Animation',\n",
       "       'Comedy', 'Family', 'Fantasy', 'Horror', 'Drama', 'Sport',\n",
       "       'Romance', 'Action', 'Sci-Fi', 'News', 'History', 'Thriller',\n",
       "       'Western', 'Crime', 'Mystery', 'Biography', 'Musical', 'War',\n",
       "       'Reality-TV'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0b2dee6-8890-413e-9829-517eef8b86c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a \"Genre\" classes:\n",
    "genre_classes = [\n",
    "    {\"label\": \"Documentary\", \"description\": \"A movie whose genre is Documentary\"},\n",
    "    {\"label\": \"Music\", \"description\": \"A movie whose genre is Music\"},\n",
    "    {\"label\": \"Adventure\", \"description\": \"A movie whose genre is Adventure\"},\n",
    "    {\"label\": \"Animation\", \"description\": \"A movie whose genre is Animation\"},\n",
    "    {\"label\": \"Comedy\", \"description\": \"A movie whose genre is Comedy\"},\n",
    "    {\"label\": \"Family\", \"description\": \"A movie whose genre is Family\"},\n",
    "    {\"label\": \"Fantasy\", \"description\": \"A movie whose genre is Fantasy\"},\n",
    "    {\"label\": \"Horror\", \"description\": \"A movie whose genre is Horror\"},\n",
    "    {\"label\": \"Drama\", \"description\": \"A movie whose genre is Drama\"},\n",
    "    {\"label\": \"Sport\", \"description\": \"A movie whose genre is Sport\"},\n",
    "    {\"label\": \"Romance\", \"description\": \"A movie whose genre is Romance\"},\n",
    "    {\"label\": \"Action\", \"description\": \"A movie whose genre is Action\"},\n",
    "    {\"label\": \"Sci-Fi\", \"description\": \"A movie whose genre is Sci-Fi\"},\n",
    "    {\"label\": \"News\", \"description\": \"A movie whose genre is News\"},\n",
    "    {\"label\": \"History\", \"description\": \"A movie whose genre is History\"},\n",
    "    {\"label\": \"Thriller\", \"description\": \"A movie whose genre is Thriller\"},\n",
    "    {\"label\": \"Western\", \"description\": \"A movie whose genre is Western\"},\n",
    "    {\"label\": \"Crime\", \"description\": \"A movie whose genre is Crime\"},\n",
    "    {\"label\": \"Mystery\", \"description\": \"A movie whose genre is Mystery\"},\n",
    "    {\"label\": \"Biography\", \"description\": \"A movie whose genre is Biography\"},\n",
    "    {\"label\": \"Musical\", \"description\": \"A movie whose genre is Musical\"},\n",
    "    {\"label\": \"War\", \"description\": \"A movie whose genre is War\"},\n",
    "    {\"label\": \"Reality-TV\", \"description\": \"A movie whose genre is Reality-TV\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b8536-70eb-48f5-882e-3975dc502130",
   "metadata": {},
   "source": [
    "Unfortunately, the available data do not provide a lot of information about each movie, except from the Title (which at times can be very informative though) and a plot summary. We will exploit both this information, combinding them into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea5694ce-1182-4998-9341-6b98556eaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a list of dictionary where each dictionary contains title and the true genre, and a concatenation\n",
    "# title and summary\n",
    "def create_text_string(title: str, summary: str) -> str:\n",
    "    return f\"\"\"Title: {title}\n",
    "Summary: {summary}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ca9b351-ed5f-475c-b230-2008819c7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie[\"Title+Summary\"] = df_movie.apply(lambda row: create_text_string(row['Title'], row['Summary']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a3003e4-0880-4dfe-b8b0-35502c176060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Title: Patton Oswalt: Annihilation\\nSummary: P...\n",
       "1    Title: New York Doll\\nSummary: A recovering al...\n",
       "Name: Title+Summary, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie[\"Title+Summary\"].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811bb50-c040-4b7b-bd56-89ca3a92c460",
   "metadata": {},
   "source": [
    "Now that we have a single column for both Title and Summary, let transform our `pandas` dataframe into a list of dictionaries with just the relevant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "732ce06e-cb6e-4f7c-abf2-a8cd8f8cca31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_main_info = [\n",
    "    {\n",
    "        \"Title\" : row[\"Title\"],\n",
    "        \"Genres\": row[\"Genres\"],\n",
    "        \"Text\": row[\"Title+Summary\"]\n",
    "    }\n",
    "    for i, row in df_movie.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a22401e7-9223-452e-9bd0-2413c0a30e66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'Patton Oswalt: Annihilation',\n",
       " 'Genres': ['Uncategorized'],\n",
       " 'Text': 'Title: Patton Oswalt: Annihilation\\nSummary: Patton Oswald, despite a personal tragedy, produces his best standup yet. Focusing on the tribulations of the Trump era and life after the loss of a loved one, Patton Oswald continues his journey to contribute joy to the world.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_main_info[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214e85e-28b5-4b28-9089-575d7cb21edd",
   "metadata": {},
   "source": [
    "## Class Embeddings\n",
    "\n",
    "We now possess all the necessary data to generate embeddings. As initial step,  we generate embeddings for the class descriptions. However, before proceeding, it's essential to create an instance of the OpenAI class and provide our `api_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94beaddc-aa61-4dd6-b23a-fb22d2643dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"conf.json\")\n",
    "conf_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b779c11e-1ee7-4091-9ba5-35a2872d4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OpenAi client\n",
    "client = OpenAI(api_key=conf_json[\"api_key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6674325-9ef1-4573-a0f2-dff25c303a5b",
   "metadata": {},
   "source": [
    "Of course, generating embeddings by through calls to the OpenAI API comes with some cost. The pricing model revolves around **tokens**, which represents fragments of word processed. Fortunately, there exists a helpful package called `tiktoken`that aids in estimating the cost for embeddings generation. First of all, we must calculate the total number of tokens generated by our embedding model for the text being processed. Then, multiply this figure by the cost per token. The latter, for the OpenAI `text-embedding-ada-002` - which is the model we will use in this tutorial - is around 0.0001 dollar per 1K tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08b29965-bf6d-4fdd-a572-6b5ce4602af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed384dbc-ea5b-4d46-bcee-393e6af9b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_tokens(documents: list) -> int:\n",
    "    '''\n",
    "    Function computing the total number of tokens of a list of documents\n",
    "\n",
    "    :params documents: list of documents\n",
    "    :returns: total tokens\n",
    "\n",
    "    '''\n",
    "    return sum(len(enc.encode(text)) for text in documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65ac9337-e054-46c9-b1f0-9f1ab80b2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_tokens = calculate_total_tokens([genre_class[\"description\"] for genre_class in genre_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d12ec09f-5d49-4998-bc4a-841ee14bf73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of Class Embeddings:  1.4100000000000002e-05\n"
     ]
    }
   ],
   "source": [
    "# estimating cost using the OpenAI Cost per token\n",
    "cost_per_1k_tokens = 0.0001\n",
    "print(\"Cost of Class Embeddings: \", cost_per_1k_tokens * class_tokens/1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8acd13-dbbb-4ea3-8632-d4619cbb6513",
   "metadata": {},
   "source": [
    "Similarly, we can estimate the cost of embedding the movie Title+Summary texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50fbe9b1-5caa-4e26-9728-cd92a2b27636",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_text_tokens = calculate_total_tokens([movie_text[\"Text\"] for movie_text in movies_main_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d538923-8221-4940-bfc8-54a03ef4b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of Movie Text Embeddings:  0.0458774\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost of Movie Text Embeddings: \", cost_per_1k_tokens * movie_text_tokens/1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda44fb6-3adc-435a-91a5-1ac9cedb80e0",
   "metadata": {},
   "source": [
    "Generating the embeddings for the text information avaiable for movies in our movie dataset costs just 4 cents. The reason is that we do not have large portion of text describing movies plot. Of course, small pieces of information may lead to poor performance of our embeddings classifier but at the same time long text descriptions may lead to high costs without real advanteges for the class predictions. \n",
    "\n",
    "Let's code a function to produce the embedding of a given text. In addition, let's also write a `timer` decorator to monitor the execution time of embeddings generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "792a45ee-34e3-4154-afd8-0ab485800b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# A function to decorate other functions to return execution time\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time = (end_time - start_time) / 60\n",
    "        print(f\"Execution time of {func.__name__}: {execution_time} minutes\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cefb5ac7-2b17-4c5a-be89-ede0938641cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a function to generate embeddings\n",
    "@timer\n",
    "def create_embeddings(text_list: List[str]) -> List[list]:\n",
    "    '''\n",
    "    Function generating the embeddings of a text_list making calls to embeddings.create OpenAI API endpoint\n",
    "\n",
    "    :params text_list: a list of text documents\n",
    "    :returns: the embedding vector as a list\n",
    "\n",
    "    '''\n",
    "\n",
    "    def get_embedding_from_openai(text: str) -> List[float]:\n",
    "        response = client.embeddings.create(\n",
    "            model = \"text-embedding-ada-002\",\n",
    "            input=text\n",
    "        )\n",
    "    \n",
    "        return response.data[0].embedding\n",
    "\n",
    "    return [get_embedding_from_openai(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bef710a0-ffe8-413f-8b17-9ae27a995e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let first create the embeddings of genres\n",
    "class_descriptions = [genre[\"description\"] for genre in genre_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d7a5670-21a0-4c2c-96d6-e36b5a65aee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of create_embeddings: 0.0998132308324178 minutes\n"
     ]
    }
   ],
   "source": [
    "class_embeddings = create_embeddings(class_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f7d8616-ce44-48fe-8c06-404f67d80932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the class_embeddings: 23\n",
      "Dimension of each embedding: 1536\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print(f\"Dimension of the class_embeddings: {len(class_embeddings)}\")\n",
    "print(f\"Dimension of each embedding: {len(class_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d5b1f-3ad8-45d0-95b6-ba89334ebd39",
   "metadata": {},
   "source": [
    "We can see that `class_embeddings`has dimension `23`, which is the number of available classes (i.e. genres). Each embedding is a 1536-dimensional vector, since the method we are using, `text-embedding-ada-002` produces embeddings of such dimensionality. To visualize embeddings and verify if semantically similar words are proximate, we can project this 1536-dimensional embedding space into a lower-dimensional space, such as a 2D Euclidean space. It's important to note that in undertaking such a projection, a considerable amount of information is lost, thus dimensionality reduction must be used with caution. Nonetheless, this approach can provide us with a qualitative understanding of how embeddings capture semantics.\n",
    "\n",
    "We can use *t-SNE* (**t-Distributed stochastic neighbor embedding**) as implemented in `sklearn` to perform such a dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2209e51-d7f4-4387-a3c8-117264a5c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define now a function to find the N closest embeddings match usine cosine distance\n",
    "def find_n_closest(query_vector, embeddings, n=3):\n",
    "    \n",
    "    distances = []\n",
    "    for index, embedding in enumerate(embeddings):\n",
    "        dist = distance.cosine(query_vector, embedding)\n",
    "        distances.append({\"distance\": dist, \"index\" : index})\n",
    "    \n",
    "    distances_sorted = sorted(distances, key=lambda x: x[\"distance\"])\n",
    "    \n",
    "    return distances_sorted[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bfee3cd9-a2d8-4208-b735-6820099e2ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's now create the embeddings a Movie in the movie list\n",
    "movie_embedding = create_embeddings([movies[10][\"Text\"]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0e8d05b-d2a8-46f3-9329-efa5bb764d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = find_n_closest(movie_embedding, class_embeddings, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6e69edf2-f911-4af9-9ea5-e2508ceb8c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'distance': 0.2086858468468431, 'index': 10},\n",
       " {'distance': 0.22009374012743632, 'index': 1},\n",
       " {'distance': 0.22295164328313177, 'index': 16}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3f320589-3da0-441b-8dea-223e5074ecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre matched: Romance, Current genres: ['Drama', 'Music', 'Romance']\n",
      "Genre matched: Music, Current genres: ['Drama', 'Music', 'Romance']\n",
      "Genre matched: Western, Current genres: ['Drama', 'Music', 'Romance']\n"
     ]
    }
   ],
   "source": [
    "for hit in hits:\n",
    "    genre_matched = genre_dict[hit[\"index\"]][\"label\"]\n",
    "    print(f\"Genre matched: {genre_matched}, Current genres: {movies[10]['Genres']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a363920-3708-44e7-b39d-d3d3a6b582cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'Forever My Girl',\n",
       " 'Genres': ['Drama', 'Music', 'Romance'],\n",
       " 'Text': 'Title: Forever My Girl\\nSummary: After being gone for a decade a country star returns home to the love he left behind.'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "446dbcdf-d6fe-4195-a970-0e595539d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let' now try to iterate over few movies and see how the model performs:\n",
    "target_movies_embeddings = create_embeddings([movie[\"Text\"] for movie in movies[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ab1c0db-bf76-4ced-9ab8-d025a37861df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 20)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_movies_embeddings[0]), len(target_movies_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca7b7d78-d706-4e3e-86cd-2478b66ab9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matched_list(hits, labels_dict):\n",
    "    return [labels_dict[hit[\"index\"]][\"label\"] for hit in hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3b310774-7e25-4cd2-a054-fc9317a1a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b20d620-dfca-4936-9c99-d3296d96987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, movie_embedding in enumerate(target_movies_embeddings):\n",
    "    hits = find_n_closest(movie_embedding, class_embeddings, n=5)\n",
    "    matched_list = create_matched_list(hits, genre_dict)\n",
    "\n",
    "    predictions_list.append([movies[i]['Title'], movies[i]['Genres'], matched_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "72fe41d1-17fc-4ec7-9c77-12c88f1782bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame(predictions_list, columns=[\"Title\", \"Current_Genres\", \"Predicted_Genres\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c1925be1-5b0c-4e40-ac08-8a23feda47b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Current_Genres</th>\n",
       "      <th>Predicted_Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patton Oswalt: Annihilation</td>\n",
       "      <td>[Uncategorized]</td>\n",
       "      <td>[Comedy, Documentary, News, Drama, History]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New York Doll</td>\n",
       "      <td>[Documentary, Music]</td>\n",
       "      <td>[Horror, Thriller, Music, Documentary, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mickey's Magical Christmas: Snowed in at the H...</td>\n",
       "      <td>[Adventure, Animation, Comedy, Family, Fantasy]</td>\n",
       "      <td>[Animation, Family, Fantasy, Comedy, Adventure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mickey's House of Villains</td>\n",
       "      <td>[Animation, Comedy, Family, Fantasy, Horror]</td>\n",
       "      <td>[Animation, Family, Fantasy, Comedy, Horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And Then I Go</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Drama, Thriller, Family, Horror, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>An Extremely Goofy Movie</td>\n",
       "      <td>[Animation, Comedy, Family, Sport]</td>\n",
       "      <td>[Comedy, Family, Animation, Sport, Adventure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Peter Rabbit</td>\n",
       "      <td>[Adventure, Animation, Comedy, Family, Fantasy]</td>\n",
       "      <td>[Animation, Family, Comedy, Fantasy, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Love Songs</td>\n",
       "      <td>[Uncategorized]</td>\n",
       "      <td>[Romance, Music, Drama, Comedy, Family]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>89</td>\n",
       "      <td>[Uncategorized]</td>\n",
       "      <td>[Sport, History, Documentary, Drama, Action]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Foster Boy</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Family, Drama, Western, Documentary, Horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Forever My Girl</td>\n",
       "      <td>[Drama, Music, Romance]</td>\n",
       "      <td>[Romance, Music, Western, Family, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tom Segura: Disgraceful</td>\n",
       "      <td>[Comedy, Documentary]</td>\n",
       "      <td>[Comedy, Documentary, News, Family, Horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Secret Rules of Modern Living: Algorithms</td>\n",
       "      <td>[Documentary]</td>\n",
       "      <td>[Sci-Fi, Fantasy, Documentary, Comedy, History]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Secrets in the Fall</td>\n",
       "      <td>[Family]</td>\n",
       "      <td>[Adventure, Thriller, Horror, Fantasy, Family]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Silent Night</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "      <td>[Family, Horror, Thriller, Drama, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Suicide Squad: Hell to Pay</td>\n",
       "      <td>[Action, Animation]</td>\n",
       "      <td>[Action, Fantasy, Sci-Fi, Horror, Thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wildling</td>\n",
       "      <td>[Fantasy, Horror]</td>\n",
       "      <td>[Horror, Thriller, Fantasy, Family, Western]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The Humanity Bureau</td>\n",
       "      <td>[Action, Sci-Fi]</td>\n",
       "      <td>[Sci-Fi, Documentary, Thriller, News, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Farewell Ferris Wheel</td>\n",
       "      <td>[Documentary, Drama, News]</td>\n",
       "      <td>[Documentary, Western, Family, Drama, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Don't Talk to Irene</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Drama, Comedy, Documentary, Sport, Romance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                         Patton Oswalt: Annihilation   \n",
       "1                                       New York Doll   \n",
       "2   Mickey's Magical Christmas: Snowed in at the H...   \n",
       "3                          Mickey's House of Villains   \n",
       "4                                       And Then I Go   \n",
       "5                            An Extremely Goofy Movie   \n",
       "6                                        Peter Rabbit   \n",
       "7                                          Love Songs   \n",
       "8                                                  89   \n",
       "9                                      The Foster Boy   \n",
       "10                                    Forever My Girl   \n",
       "11                            Tom Segura: Disgraceful   \n",
       "12      The Secret Rules of Modern Living: Algorithms   \n",
       "13                                Secrets in the Fall   \n",
       "14                                       Silent Night   \n",
       "15                         Suicide Squad: Hell to Pay   \n",
       "16                                           Wildling   \n",
       "17                                The Humanity Bureau   \n",
       "18                              Farewell Ferris Wheel   \n",
       "19                                Don't Talk to Irene   \n",
       "\n",
       "                                     Current_Genres  \\\n",
       "0                                   [Uncategorized]   \n",
       "1                              [Documentary, Music]   \n",
       "2   [Adventure, Animation, Comedy, Family, Fantasy]   \n",
       "3      [Animation, Comedy, Family, Fantasy, Horror]   \n",
       "4                                           [Drama]   \n",
       "5                [Animation, Comedy, Family, Sport]   \n",
       "6   [Adventure, Animation, Comedy, Family, Fantasy]   \n",
       "7                                   [Uncategorized]   \n",
       "8                                   [Uncategorized]   \n",
       "9                                           [Drama]   \n",
       "10                          [Drama, Music, Romance]   \n",
       "11                            [Comedy, Documentary]   \n",
       "12                                    [Documentary]   \n",
       "13                                         [Family]   \n",
       "14                                  [Comedy, Drama]   \n",
       "15                              [Action, Animation]   \n",
       "16                                [Fantasy, Horror]   \n",
       "17                                 [Action, Sci-Fi]   \n",
       "18                       [Documentary, Drama, News]   \n",
       "19                                         [Comedy]   \n",
       "\n",
       "                                   Predicted_Genres  \n",
       "0       [Comedy, Documentary, News, Drama, History]  \n",
       "1   [Horror, Thriller, Music, Documentary, Fantasy]  \n",
       "2   [Animation, Family, Fantasy, Comedy, Adventure]  \n",
       "3      [Animation, Family, Fantasy, Comedy, Horror]  \n",
       "4        [Drama, Thriller, Family, Horror, Fantasy]  \n",
       "5     [Comedy, Family, Animation, Sport, Adventure]  \n",
       "6     [Animation, Family, Comedy, Fantasy, Romance]  \n",
       "7           [Romance, Music, Drama, Comedy, Family]  \n",
       "8      [Sport, History, Documentary, Drama, Action]  \n",
       "9     [Family, Drama, Western, Documentary, Horror]  \n",
       "10         [Romance, Music, Western, Family, Drama]  \n",
       "11      [Comedy, Documentary, News, Family, Horror]  \n",
       "12  [Sci-Fi, Fantasy, Documentary, Comedy, History]  \n",
       "13   [Adventure, Thriller, Horror, Fantasy, Family]  \n",
       "14       [Family, Horror, Thriller, Drama, Romance]  \n",
       "15      [Action, Fantasy, Sci-Fi, Horror, Thriller]  \n",
       "16     [Horror, Thriller, Fantasy, Family, Western]  \n",
       "17     [Sci-Fi, Documentary, Thriller, News, Drama]  \n",
       "18   [Documentary, Western, Family, Drama, Fantasy]  \n",
       "19     [Drama, Comedy, Documentary, Sport, Romance]  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18732d9b-eb43-4aff-a544-b45a8d9fdb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b479f7-0a93-4b30-aa39-a9d9b1635b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
